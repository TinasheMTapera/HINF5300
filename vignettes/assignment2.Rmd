---
title: "Assignment 2: Activity Recognition"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Assignment 2: Activity Recognition}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(HINF5300)

library(dplyr)
library(readr)
library(lubridate)
library(here)
library(stringr)
library(tidyr)
library(purrr)
library(skimr)
library(ggplot2)
library(forcats)
library(tidymodels)
library(slider)
library(furrr)
```

In assignment 2, we're working on activity recognition. Given a large set of
activity data from multiple participants, can we build a machine learning model
that can detect what physical activity they are engaged in?

## The Data

The data comes from the [WIreless Sensor Data Mining group](https://www.cis.fordham.edu/wisdm/dataset.php)
from Fordham University^[[Kwapisz et. al, 2011](https://doi.org/10.1145/1964897.1964918)]. We can get it from here:

```{sh, eval=FALSE}
cd ../inst/extdata && curl https://www.cis.fordham.edu/wisdm/includes/datasets/latest/WISDM_ar_latest.tar.gz -O -L

tar -xzvf WISDM_ar_latest.tar.gz

mv WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt activity.csv

rm -rf WISDM*
```

There's quite a bit of wonky lines in this file; this little piece of code
takes care of most of it:

```{r}
# loading it dynamically since it's not installed
lines <- read_lines(here("inst", "extdata", "activity.csv"))

# lines are ended with ";", but for some reason some lines have two entries
# so we gather the lines with a ";" followed by any other character,
# split them, and reduce+combine them
lines_unsep <- lines %>%
  str_subset(";.") %>%
  str_split(";(?=.)") %>%
  reduce(append)

# these lines are normal... hopefully
lines_sep <- lines %>%
  str_subset(";.", negate = TRUE)

df <- c(lines_sep, lines_unsep) %>%
  str_remove_all(";") %>%
  str_remove(",$") %>%
  tibble(x = .) %>%
  filter(x != "") %>% # i think empty rows got created by all our string manipulation # nolint
  separate(x,
  into = c(
    'user', 'activity', 'timestamp',
    'x_axis', 'y_axis','z_axis'), sep = ",") %>%
  mutate(across(c(timestamp, contains("axis")), as.numeric)) %>%
  mutate(user = as.factor(user), activity = as.factor(activity))
```

Let's do some EDA

```{r}
skim(df)
```

Looks like there is one row of missing data from the Z axis that we'll remove:

```{r}
df %>%
  filter(is.na(z_axis))

df <- df %>%
  filter(!is.na(z_axis))
```

And a few rows where timestamp is 0:

```{r}
df <- df %>%
  filter(timestamp != 0)
```



We can also look at the distributions of the accelerometer data:

```{r}
df %>%
  select(contains("axis")) %>%
  pivot_longer(everything()) %>%
  ggplot(aes(x=value, fill=name)) +
  geom_density(alpha=0.5) +
  theme_minimal()
```

The Y axis is slightly skewed, but not awful. I'm assuming that's an axis that gets a lot of activity like in someone's pocket:

```{r}
df %>%
  select(activity, y_axis) %>%
  ggplot(aes(x=y_axis, fill=activity)) +
  geom_density(alpha = 0.5) +
  theme_minimal()
```

Jogging; makes sense!

We can look at the classes next:

```{r}
df %>%
  count(activity) %>%
  mutate(activity = fct_reorder(activity, n)) %>%
  ggplot(aes(y=n, x=activity)) +
  geom_col() +
  theme_minimal()
```

So walking and jogging seem to be the highest recorded factors, by quite a bit.
Downstairs and upstairs are essentially the same â€” I'm considering putting them
into the same category as "stairs". Likewise, standing and sitting just constitute
stationary-ness. A multi-class classifier would definitely do better if the classes
are equal, so maybe this will help:

```{r}
df %>%
  mutate(activity = fct_collapse(
    activity, 
    Stationary = c("Standing", "Sitting"),
    Stairs = c("Downstairs", "Upstairs"))) %>%
  count(activity) %>%
  mutate(activity = fct_reorder(activity, n)) %>%
  ggplot(aes(y=n, x=activity)) +
  geom_col() +
  theme_minimal()
```

This might be easier for a model to handle; we'll save this as another variable
and compare our results to see if it's worth it.

```{r}
df <- df %>%
  mutate(activity_binned = fct_collapse(
    activity, 
    Stationary = c("Standing", "Sitting"),
    Stairs = c("Downstairs", "Upstairs"))
    )
```

How does the data compare across the 36 users?

```{r}
df %>%
  group_by(user) %>%
  summarise(n_rows = n()) %>%
  mutate(user = fct_reorder(user, n_rows)) %>%
  ggplot(aes(y=n_rows, x=user)) +
  geom_col() +
  theme_minimal()
```

So some users have twice (almost three times) more data than others. We
might need to stratify sampling if predictions are poor.

Let's check the timestamps:

```{r}
df %>%
  group_by(user) %>%
  arrange(timestamp) %>%
  ggplot(aes(x=user, y=timestamp)) +
  geom_point(position = "jitter", alpha=0.1) +
  theme_minimal()
```

I'm not a fan of this Android timestamp thing, I must say. 

Let's make sure the rows are unique. This step additionally groups
the data by user and activity and arranges the data
by timestamp.

```{r}
df <- df %>%
  group_by(user, activity) %>%
  arrange(user, activity, timestamp) %>%
  ungroup() %>%
  distinct(across(everything()) , .keep_all=TRUE)
```

Another way to visualize data for a handful of participants:

```{r}
df %>%
  group_by(user) %>%
  nest() %>%
  ungroup() %>%
  slice_sample(n=3) %>%
  unnest(data) %>%
  ggplot(aes(x=timestamp, y=activity)) +
  geom_point(aes(color=user),show.legend = FALSE) +
  theme_minimal()
```

One last sanity check:

```{r}
df %>%
  group_by(user) %>%
  nest() %>%
  ungroup() %>%
  slice_sample(n=3) %>%
  unnest(data) %>%
  #select(contains("axis")) %>%
  pivot_longer(cols=-c(user, activity, activity_binned, timestamp)) %>%
  ggplot(aes(x=value, fill=name)) +
  geom_density(alpha=0.5) +
  facet_grid(activity ~ user, scales="free") +
  theme_minimal()
```

Let's see if we can fix the timestamp; we're told:

> timestamp: generally the phone's uptime in nanoseconds (In future datasets this will
be miliseconds since unix epoch.)
Sampling rate: 20Hz (1 sample every 50ms)

So we should convert this to make a zero reading at the beginning of each epoch,
and then measure anything where the diff is larger than 1 minute

```{r}
tmp_df <- df %>%
  group_by(user, activity) %>%
  arrange(user, activity, timestamp) %>%
  mutate(timestamp_ms = timestamp/1000000) %>% 
  mutate(timestamp_diff = timestamp_ms - lag(timestamp_ms)) %>%
  mutate(epoch = timestamp_diff > 1000 | is.na(timestamp_diff), index=1:n())

epoch_start <- which(tmp_df$epoch == TRUE, arr.ind = TRUE) %>%
  data.frame(index=., epoch_index=1:length(.))


df_clean <- tmp_df %>%
  left_join(epoch_start) %>%
  fill(epoch_index, .direction = "down") %>%
  select(user:timestamp_ms, epoch=epoch_index)
```

Let's move on before we get too bogged down in these details.

We're going to try out building the model pipeline with
Tidymodels, so we're going to need to build a preprocessing
recipe.

## Data Filtering

The first step is to filter the accelerometer data. We're going to filter the full time series' magnitude as in the previous assignment:

```{r}
df_clean <- df_clean %>%
  mutate(mag = sqrt((x_axis^2 + y_axis^2 + z_axis^2))) %>%
    mutate(clean_signal = mag) %>%
           #filter_signal()) %>%
           #smooth_signal()) %>%
  filter(!is.na(clean_signal)) %>%
  mutate(timestamp_dt = as.POSIXct(timestamp_ms/1000, origin = "1960-01-01")) %>%
  select(user, activity, activity_binned, epoch, timestamp_dt, x_axis:z_axis, mag, clean_signal)
  
```

## Data Segmentation

Next, we'll break the data down into segments that'll be used to
generate features. We're using non-overlapping, 10-second segments:

```{r}
get_segments <- function(segment, column){
  slide_period(
    segment,
    .i=segment$timestamp_dt,
    .period="second",
    .f = ~ .x %>% select({{ column }}),
    .every=10,
    complete=FALSE
  )
}

df_segmented <- df_clean %>%
  group_by(epoch, .add=TRUE) %>%
  nest() %>%
  ungroup() %>%
  mutate(
    segments_x = map(data, .f = get_segments, x_axis),
    segments_y = map(data, .f = get_segments, y_axis),
    segments_z = map(data, .f = get_segments, z_axis),
    segments_mag = map(data, .f = get_segments, mag)
  ) %>%
  select(-data) %>%
  unnest(c(segments_x, segments_y, segments_z, segments_mag))
```

We now have `r nrow(df_segmented)` segments,
which will translate to a machine learning problem with that
many rows for training and testing. We'll use each of these
segments and create a set of features with Varun's Python code.

```{r}
get_features <- function(
  vec,
  type,
  script = here("inst", "py", "extract_features.py"),
  feature_names = c(
    "mean", "standard_deviation", "avg_abs_difference",
    "min", "max", "max_min_differrence",
    "median", "med_abs_deviation",
    "iqr", "negative_cnt", 'positive_cnt',
    "vals_above_mean", "n_peaks",
    "skewness", "kurtosis", "energy", "signal_area"
    ),
    print_debug=FALSE
  ) {

  ## step 1: write the vector of signals to a temporary file

  this_tempfile <- tempfile()

  vec %>% write_delim(this_tempfile, col_names = FALSE)

  ## step 2: call the python script and have it write to file

  cmd <- paste0(c("source /opt/homebrew/Caskroom/miniconda/base/etc/profile.d/conda.sh &&", 
  "conda activate hinf5300 && python", 
  script, this_tempfile, type), sep=" ", collapse=" ")

  if(print_debug) print(paste("running:", cmd))

  out <- system(cmd, intern = TRUE, ignore.stdout = TRUE, ignore.stderr = TRUE)

  ## step 3: read in the file that was written by python
  if(type == "frequency"){
    feature_names <- c("dc_component", feature_names)
  }

  features <- read_lines(this_tempfile) %>%
    as.numeric() %>%
    data.frame() %>%
    t()

  colnames(features) <- feature_names

  ## step 4: gc
  unlink(this_tempfile)

  ## return
  return(as_tibble(features))

}
```

This chunk take a short while to run, plut it calls Python in the
background, so I've let it precompute and saved the outputs.

```{r}
library(tictoc)

tic()
plan(multisession, workers = 8)

df_features <- df_segmented %>%
  slice_sample(n=50) %>%
  mutate(
    time_x = future_map(segments_x, function(x) get_features(x, type = "time")),
    freq_x = future_map(segments_x, function(x) get_features(x, type = "frequency")),
    time_y = future_map(segments_y, function(x) get_features(x, type = "time")),
    freq_y = future_map(segments_y, function(x) get_features(x, type = "frequency")),
    time_z = future_map(segments_z, function(x) get_features(x, type = "time")),
    freq_z = future_map(segments_z, function(x) get_features(x, type = "frequency")),
    time_mag = future_map(segments_mag, function(x) get_features(x, type = "time")),
    freq_mag = future_map(segments_mag, function(x) get_features(x, type = "frequency"))) %>%
  unnest(matches("time_|freq_"), names_sep = ".") %>%
  select(-contains("segments"))
toc()
```




