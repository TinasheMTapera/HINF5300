---
title: "Activity Recognition Part 1: Preprocessing"
subtitle: "Data Preprocessing & Feature Extraction"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Activity Recognition Part 1: Preprocessing}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(HINF5300)

library(dplyr)
library(readr)
library(lubridate)
library(here)
library(stringr)
library(tidyr)
library(purrr)
library(skimr)
library(ggplot2)
library(forcats)
library(tidymodels)
library(slider)
```

In assignment 2, we're working on activity recognition. Given a large set of
activity data from multiple participants, can we build a machine learning model
that can detect what physical activity they are engaged in?

This is the first part of a 2 part machine learning task. This notebook takes care of
data exploration, preprocessing, and feature extraction.

## The Data

The data comes from the [WIreless Sensor Data Mining group](https://www.cis.fordham.edu/wisdm/dataset.php)
from Fordham University^[[Kwapisz et. al, 2011](https://doi.org/10.1145/1964897.1964918)]. We can get it from here:

```{sh, eval=FALSE}
cd ../inst/extdata && curl https://www.cis.fordham.edu/wisdm/includes/datasets/latest/WISDM_ar_latest.tar.gz -O -L

tar -xzvf WISDM_ar_latest.tar.gz

mv WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt activity.csv

rm -rf WISDM*
```

There's quite a bit of wonky lines in this file; this little piece of code
takes care of most of it:

```{r}
# loading it dynamically since it's not installed
lines <- read_lines(here("inst", "extdata", "activity.csv"))

# lines are ended with ";", but for some reason some lines have two entries
# so we gather the lines with a ";" followed by any other character,
# split them, and reduce+combine them
lines_unsep <- lines %>%
  str_subset(";.") %>%
  str_split(";(?=.)") %>%
  reduce(append)

# these lines are normal... hopefully
lines_sep <- lines %>%
  str_subset(";.", negate = TRUE)

df <- c(lines_sep, lines_unsep) %>%
  str_remove_all(";") %>%
  str_remove(",$") %>%
  tibble(x = .) %>%
  filter(x != "") %>% # i think empty rows got created by all our string manipulation # nolint
  separate(x,
  into = c(
    'user', 'activity', 'timestamp',
    'x_axis', 'y_axis','z_axis'), sep = ",") %>%
  mutate(across(c(timestamp, contains("axis")), as.numeric)) %>%
  mutate(user = as.factor(user), activity = as.factor(activity))
```

## EDA & Data Cleaning

Let's do some EDA

```{r}
skim(df)
```

Looks like there is one row of missing data from the Z axis that we'll remove:

```{r}
df %>%
  filter(is.na(z_axis))

df <- df %>%
  filter(!is.na(z_axis))
```

And a few rows where timestamp is 0:

```{r}
df <- df %>%
  filter(timestamp != 0)
```


We can also look at the distributions of the accelerometer data:

```{r}
df %>%
  select(contains("axis")) %>%
  pivot_longer(everything()) %>%
  ggplot(aes(x=value, fill=name)) +
  geom_density(alpha=0.5) +
  theme_minimal()
```

The Y axis is slightly skewed, but not awful. I'm assuming that's an axis that gets a lot of activity like in someone's pocket:

```{r}
df %>%
  select(activity, y_axis) %>%
  ggplot(aes(x=y_axis, fill=activity)) +
  geom_density(alpha = 0.5) +
  theme_minimal()
```

Jogging; makes sense!

We can look at the classes next:

```{r}
df %>%
  count(activity) %>%
  mutate(activity = fct_reorder(activity, n)) %>%
  ggplot(aes(y=n, x=activity)) +
  geom_col() +
  theme_minimal()
```

So walking and jogging seem to be the highest recorded factors, by quite a bit.
Downstairs and upstairs are essentially the same â€” I'm considering putting them
into the same category as "stairs". Likewise, standing and sitting just constitute
stationary-ness. A multi-class classifier would definitely do better if the classes
are equal, so maybe this will help:

```{r}
df %>%
  mutate(activity = fct_collapse(
    activity, 
    Stationary = c("Standing", "Sitting"),
    Stairs = c("Downstairs", "Upstairs"))) %>%
  count(activity) %>%
  mutate(activity = fct_reorder(activity, n)) %>%
  ggplot(aes(y=n, x=activity)) +
  geom_col() +
  theme_minimal()
```

This might be easier for a model to handle; we'll save this as another variable
and compare our results to see if it's worth it.

```{r}
df <- df %>%
  mutate(activity_binned = fct_collapse(
    activity, 
    Stationary = c("Standing", "Sitting"),
    Stairs = c("Downstairs", "Upstairs"))
    )
```

How does the data compare across the 36 users?

```{r}
df %>%
  group_by(user) %>%
  summarise(n_rows = n()) %>%
  mutate(user = fct_reorder(user, n_rows)) %>%
  ggplot(aes(y=n_rows, x=user)) +
  geom_col() +
  theme_minimal()
```

So some users have twice (almost three times) as much data than others. We
might need to stratify sampling if predictions are poor.

Let's check the timestamps:

```{r}
df %>%
  group_by(user) %>%
  arrange(timestamp) %>%
  ggplot(aes(x=user, y=timestamp)) +
  geom_point(position = "jitter", alpha=0.1) +
  theme_minimal()
```

I'm not a fan of this Android timestamp thing, I must say. 

Let's make sure the rows are unique. This step additionally groups
the data by user and activity and arranges the data
by timestamp.

```{r}
df <- df %>%
  group_by(user, activity) %>%
  arrange(user, activity, timestamp) %>%
  ungroup() %>%
  distinct(across(everything()) , .keep_all=TRUE)
```

Another way to visualize data for a handful of participants:

```{r}
df %>%
  group_by(user) %>%
  nest() %>%
  ungroup() %>%
  slice_sample(n=3) %>%
  unnest(data) %>%
  ggplot(aes(x=timestamp, y=activity)) +
  geom_point(aes(color=user),show.legend = FALSE) +
  theme_minimal()
```

One last sanity check:

```{r}
df %>%
  group_by(user) %>%
  nest() %>%
  ungroup() %>%
  slice_sample(n=3) %>%
  unnest(data) %>%
  pivot_longer(cols=-c(user, activity, activity_binned, timestamp)) %>%
  ggplot(aes(x=value, fill=name)) +
  geom_density(alpha=0.5) +
  facet_grid(activity ~ user, scales="free") +
  theme_minimal()
```

### Creating Epochs with Timestamps

Let's see if we can fix the timestamp; we're told:

> timestamp: generally the phone's uptime in nanoseconds (In future datasets this will
be miliseconds since unix epoch.)
Sampling rate: 20Hz (1 sample every 50ms)

So we should convert this to make a zero reading at the beginning of each epoch,
and then measure anything where the diff is larger than 1 minute:

```{r}
tmp_df <- df %>%
  group_by(user, activity) %>%
  arrange(user, activity, timestamp) %>%
  mutate(timestamp_ms = timestamp/1000000) %>% 
  mutate(timestamp_diff = timestamp_ms - lag(timestamp_ms)) %>%
  mutate(epoch = timestamp_diff > 1000 | is.na(timestamp_diff), index=1:n())

epoch_start <- which(tmp_df$epoch == TRUE, arr.ind = TRUE) %>%
  data.frame(index=., epoch_index=1:length(.))


df_clean <- tmp_df %>%
  left_join(epoch_start) %>%
  fill(epoch_index, .direction = "down") %>%
  select(user:timestamp_ms, epoch=epoch_index)
```

I wanted to do this so that when we create segments in the data, we can 
prevent overlapping of data from one task to the next (i.e. even if we sort
by user and timestamp, there's a chance the same user could have done a jogging 
task, stopped, and started over again; I don't want there to be overlaps in that sense).

Let's move on before we get too bogged down in these details.

## Data Filtering

The first step is to filter the accelerometer data. We're going to use the full time series' magnitude as in the previous assignment:

**We actually decide _not_ to filter and smooth the data.**

```{r}
df_clean <- df_clean %>%
  mutate(mag = sqrt((x_axis^2 + y_axis^2 + z_axis^2))) %>%
    mutate(clean_signal = mag) %>%
           #filter_signal()) %>%
           #smooth_signal()) %>%
  filter(!is.na(clean_signal)) %>%
  mutate(timestamp_dt = as.POSIXct(timestamp_ms/1000, origin = "1960-01-01")) %>%
  select(user, activity, activity_binned, epoch, timestamp_dt, x_axis:z_axis, mag, clean_signal)
  
```

## Data Segmentation

Next, we'll break the data down into segments that'll be used to
generate features. We're using non-overlapping, 10-second segments:

```{r}
get_segments <- function(segment, column){
  slide_period(
    segment,
    .i=segment$timestamp_dt,
    .period="second",
    .f = ~ .x %>% select({{ column }}),
    .every=10,
    complete=FALSE
  )
}

df_segmented <- df_clean %>%
  group_by(epoch, .add=TRUE) %>%
  nest() %>%
  ungroup() %>%
  mutate(
    segments_x = map(data, .f = get_segments, x_axis),
    segments_y = map(data, .f = get_segments, y_axis),
    segments_z = map(data, .f = get_segments, z_axis),
    segments_mag = map(data, .f = get_segments, mag)
  ) %>%
  select(-data) %>%
  unnest(c(segments_x, segments_y, segments_z, segments_mag))

# quickly filter
df_segmented <- df_segmented %>%
  mutate(
    has_small_segment_x = map_lgl(segments_x, function(x) nrow(x) < 10),
    has_small_segment_y = map_lgl(segments_y, function(x) nrow(x) < 10),
    has_small_segment_z = map_lgl(segments_z, function(x) nrow(x) < 10),
    has_small_segment_mag = map_lgl(segments_mag, function(x) nrow(x) < 10)) %>%
  filter(!if_any(contains("has_small_segment"))) %>%
  select(-contains("has_small_segment"), -epoch)
```

We now have `r nrow(df_segmented)` segments,
which will translate to a machine learning problem with that
many rows for training and testing. We'll use each of these
segments and create a set of features with Varun's Python code.

```{r}
get_features <- function(
  vec,
  type,
  script = here("inst", "py", "extract_features.py"),
  feature_names = c(
    "mean", "standard_deviation", "avg_abs_difference",
    "min", "max", "max_min_differrence",
    "median", "med_abs_deviation",
    "iqr", "negative_cnt", 'positive_cnt',
    "vals_above_mean", "n_peaks",
    "skewness", "kurtosis", "energy", "signal_area"
    ),
    print_debug=FALSE
  ) {

    reticulate::source_python(here::here("inst", "py", "assign2_extract-features.py"))

    vec <- vec %>%
      pull(1)

    if(type == "frequency"){
      feature_names <- c("dc_component", feature_names)
      features <- get_freq_domain_features(vec)
    } else {
      features <- get_time_series_features(vec)
    }

    features <- data.frame(unlist(features)) %>% t()
    colnames(features) <- feature_names

    return(as_tibble(features))
  }
```

This chunk take a short while to run, plus it calls Python in the
background, so I've let it precompute and saved the outputs.

```{r, eval=FALSE}
library(tictoc)

tic()

df_features <- df_segmented %>%
  mutate(
    time_x = map(segments_x, function(x) get_features(x, type = "time")),
    freq_x = map(segments_x, function(x) get_features(x, type = "frequency")),
    time_y = map(segments_y, function(x) get_features(x, type = "time")),
    freq_y = map(segments_y, function(x) get_features(x, type = "frequency")),
    time_z = map(segments_z, function(x) get_features(x, type = "time")),
    freq_z = map(segments_z, function(x) get_features(x, type = "frequency")),
    time_mag = map(segments_mag, function(x) get_features(x, type = "time")),
    freq_mag = map(segments_mag, function(x) get_features(x, type = "frequency"))) %>%
  unnest(matches("time_|freq_"), names_sep = "_") %>%
  select(-contains("segments"))
toc()
```

The above is not rendered for the sake of package build time, but
it took about 7 minutes.

Now, we write this data to file:

```{r, eval=FALSE}
usethis::use_data(df_features)
```

Awesome; the adventure continues in part 2...
